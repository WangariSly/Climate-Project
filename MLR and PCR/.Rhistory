mortality<-c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
exposure<-c(2.0, 2.5, 2.7, 2.3, 2.5, 2.8, 2.4, 2.6, 2.0, 2.0,
2.4, 2.8, 2.4, 3.2, 3.5, 4.8, 6.2, 4.2, 3.6, 3.4, 4.3, 2.8,
3.2, 3.8, 3.0, 4.5, 3.8)
mortality<-c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
boxplot(exposure âˆ¼ mortality)
boxplot(exposure~mortality)
plot(mortality~xposure)
plot(mortality~exposure)
a<-plot(mortality~exposure)
abline(a)
line(a)
abline(lm(mortality~exposure))
glm(mortality~exposure,family = 'binomial')
abline(glm(mortality~exposure,family = 'binomial'))
line(glm(mortality~exposure,family = 'binomial'))
rm(list=ls())
#setwd("C:/Users/julia/OneDrive - Universita degli Studi Roma Tre/Cameroon/AIMS 2022/Slides/Rcode")require(graphics)
####Plant mortality
exposure<-c(2.0, 2.5, 2.7, 2.3, 2.5, 2.8, 2.4, 2.6, 2.0, 2.0, 2.4, 2.8, 2.4, 3.2, 3.5, 4.8,
6.2, 4.2, 3.6, 3.4, 4.3, 2.8, 3.2, 3.8, 3.0, 4.5, 3.8)
mortality<-c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1)
plot(mortality~exposure, pch = 16)
boxplot(exposure~mortality)
glm1<-glm(mortality~exposure, family="binomial")
summary(glm1)
###If we did a linear regression we can see why it would be wrong
lin<-lm(mortality~exposure)
plot(lm(mortality~exposure))
#Coefficients:
#Estimate Std. Error z value Pr(>|z|)
#(Intercept)   -7.959      2.960  -2.689  0.00717 **
#  exposure     2.584      0.984   2.626  0.00865 **
#pi<-exp(-7.969+  2.584* exposure)/(1+exp(-7.969+ 2.584* exposure))
X11()
#Coefficients:
#Estimate Std. Error z value Pr(>|z|)
#(Intercept)   -7.959      2.960  -2.689  0.00717 **
#  exposure     2.584      0.984   2.626  0.00865 **
#pi<-exp(-7.969+  2.584* exposure)/(1+exp(-7.969+ 2.584* exposure))
#X11()
plot(mortality~exposure, pch = 16)
lines(exposure, fitted(glm1))
fitted(glm1)
lines(mortality~exposure)
rm(list=ls())#clearing the environment
setwd("~/Documents/Essay with Prof Babatunde/prac/MLR and PCR")
getwd()
#load pls
library(pls)
#importing the data
df<-read.csv("data1 Maize.csv",sep = ',')
# Scale the columns from the second column to the last column
df[, 2:ncol(df)] <- scale(df[, 2:ncol(df)])
boxplot(df[,5])
names(df)#features
attach(df)
df <- df[,-1]#dropping the years column
head(df)#view first 6 rows
dim(df)#dimension of our dataset
str(df)
sum(is.null(df))#checking for the missing values
# outliers<-boxplot(df[,2],plot=FALSE)$out
# df<-df[-which(df[,2] %in% outliers),]
par(mfrow = c(1, 2))
set.seed(100)
#partitioning the data 70% training 30% testing
df1<-sort(sample(nrow(df), nrow(df)*0.7, replace = F))
train <- df[df1,]
test <- df[-df1,]
dim(train)
dim(test)
#looking for the correlation among variables
library(correlation)
correlation(df)
res <- cor(train, method="pearson")
corrplot::corrplot(res, method= "color", order = "hclust", tl.pos = 'n')
# Fit MLR model
# Fit the linear regression model
mlr <- lm(Maize.yield ~ ., data = train)
# Print the model summary
summary(mlr)
# Predict using the linear regression model
mlr_pred <- predict(mlr, test)
mlr_pred
par(mfrow = c(1, 2))
set.seed(123)
# Get the predicted and actual values for the scatter plot
pred <- as.numeric(mlr_pred)
actual <- test$Maize.yield
# Fit a linear model and get the intercept and slope of the line of best fit for the predicted line
model_pred <- lm(pred ~ actual)
intercept_pred <- coef(model_pred)[1]
slope_pred <- coef(model_pred)[2]
# Fit a linear model and get the intercept and slope of the line of best fit for the actual line
model_actual <- lm(actual ~ pred)
intercept_actual <- coef(model_actual)[1]
slope_actual <- coef(model_actual)[2]
# Set the aspect ratio to 1 to create a square plot
ran <- range(c(pred, actual))
xlim <- ran
ylim <- ran
plot(actual, pred, xlab = "Actual", ylab = "Predicted", main = "Maize yield", xlim = xlim, ylim = ylim)
# Add the line of best fit for the predicted line to the plot
abline(a = intercept_pred, b = slope_pred, col = 'black')
# Add the line of best fit for the actual line to the plot
abline(a = intercept_actual, b = slope_actual, col = 'red')
rm(list=ls())#clearing the environment
setwd("~/Documents/Essay with Prof Babatunde/prac/MLR and PCR")
getwd()
#load pls
library(pls)
#importing the data
df<-read.csv("data1 Maize.csv",sep = ',')
# Scale the columns from the second column to the last column
df[, 2:ncol(df)] <- scale(df[, 2:ncol(df)])
boxplot(df[,5])
names(df)#features
attach(df)
df <- df[,-1]#dropping the years column
head(df)#view first 6 rows
dim(df)#dimension of our dataset
str(df)
names(df)#features
attach(df)
df <- df[,-1]#dropping the years column
head(df)#view first 6 rows
dim(df)#dimension of our dataset
str(df)
sum(is.null(df))#checking for the missing values
par(mfrow = c(1, 2))
set.seed(100)
#partitioning the data 70% training 30% testing
df1<-sort(sample(nrow(df), nrow(df)*0.7, replace = F))
train <- df[df1,]
test <- df[-df1,]
dim(train)
dim(test)
#looking for the correlation among variables
library(correlation)
correlation(df)
res <- cor(train, method="pearson")
corrplot::corrplot(res, method= "color", order = "hclust", tl.pos = 'n')
# Fit MLR model
# Fit the linear regression model
mlr <- lm(Maize.yield ~ ., data = train)
# Print the model summary
summary(mlr)
# Predict using the linear regression model
mlr_pred <- predict(mlr, test)
mlr_pred
par(mfrow = c(1, 2))
set.seed(123)
